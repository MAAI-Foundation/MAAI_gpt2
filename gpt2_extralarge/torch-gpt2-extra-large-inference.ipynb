{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed13ec-a7c1-4d16-987f-88575b5e2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install --upgrade unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37f01899-f412-4977-9664-7ae474f54a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-09 13:55:29--  https://jpdata1-my.sharepoint.com/:u:/g/personal/anand_jpdata_co/EVQs7sQlMxRHvHBMhonrXuIBSdosZ5CQPn2S6WG4I1uuhg?e=Ncv8WW&download=1\n",
      "Resolving jpdata1-my.sharepoint.com (jpdata1-my.sharepoint.com)... 13.107.136.10, 13.107.138.10, 2620:1ec:8f8::10, ...\n",
      "Connecting to jpdata1-my.sharepoint.com (jpdata1-my.sharepoint.com)|13.107.136.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /personal/anand_jpdata_co/Documents/gpt-weights/MAAI_gpt2_extra_large_torch-50?ga=1 [following]\n",
      "--2025-10-09 13:55:29--  https://jpdata1-my.sharepoint.com/personal/anand_jpdata_co/Documents/gpt-weights/MAAI_gpt2_extra_large_torch-50?ga=1\n",
      "Reusing existing connection to jpdata1-my.sharepoint.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6281147170 (5.8G) [application/octet-stream]\n",
      "Saving to: ‘MAAI_gpt2_extra_large_torch_50’\n",
      "\n",
      "MAAI_gpt2_extra_lar 100%[===================>]   5.85G  70.7MB/s    in 81s     \n",
      "\n",
      "2025-10-09 13:56:50 (74.3 MB/s) - ‘MAAI_gpt2_extra_large_torch_50’ saved [6281147170/6281147170]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget --no-check-certificate \"https://jpdata1-my.sharepoint.com/:u:/g/personal/anand_jpdata_co/EVQs7sQlMxRHvHBMhonrXuIBSdosZ5CQPn2S6WG4I1uuhg?e=Ncv8WW&download=1\" -O MAAI_gpt2_extra_large_torch_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "443f4b90-d4f1-48ba-a468-46d2506382c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF = expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "#!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'\n",
    "# --- 1. Configuration ---\n",
    "# You can adjust these parameters\n",
    "MODEL_NAME = \"gpt2-xl\"\n",
    "MAX_LENGTH = 1024 # Maximum sequence length for GPT-2 Medium\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 1\n",
    "NUM_WORKERS = 4 # For faster data loading\n",
    "PATH= 'MAAI_gpt2_extra_large_torch_50'\n",
    "\n",
    "# --- 2. Device Setup (Essential for PyTorch) ---\n",
    "# Automatically detects and uses GPU (CUDA/ROCm) or falls back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed63344f-499c-4b04-9fd2-d02d5aea8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Initialization ---\n",
    "\n",
    "# Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Set the padding token, which is necessary for batching\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Model and move it to the device (GPU or CPU)\n",
    "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Use the lines below for loading stored weights and comment the line above\n",
    "model = torch.load(PATH, weights_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "265dba30-e338-4696-a995-5b1757de22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"तुमच्या\"\n",
    "max_new_tokens = 1024\n",
    "\n",
    "# 1. Tokenize input and move to device\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd586aac-3082-4097-8112-f629aba2ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating continuation for prompt: 'तुमच्या'\n",
      "--------------------\n",
      "Generated Text:\n",
      "तुमच्या ताकदीवर अनेक शब्द घेत असेल तर याचे पलन करता येते. मी इतका आहे की त्याचा विचार करून सर्व पक्षातील सोबत येतो. येतो तेव्हा लोकं संकटलेले असताना ते माझ्यासमोर फार कमी बोलतात. त्यामुळे लोकांचे नुकसान होत असते. आमच्यापासून ते शब्द चालत आहे. माझ्यामुळे मी येतो आणि तेथे येतो निर्णय घेतला असता तर अनेक नेत्यांनी स्वतःच्या बाजूला भाव निर्माण केला. कारण असता तर पाच वर्षातील मुलीच्या घरात फार आहे. आम्ही काम करता आणि त्याने पालकमंत्र्यांना स्थगिती दिली. ज्याची निष्ठा होती त्याची विचारधारा गाडीखाली आहे. विचारधारा आणि वेगवेगळ्या पक्षातून हे दोन गट पडत असतात अशी भावना अनेक मंत्र्यांची झाली. मात्र अध्यक्षांच्या हातात ही माहिती जमा पडते. त्यानुसार प्रकाश आंबेडकर या\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate text\n",
    "print(f\"Generating continuation for prompt: '{prompt}'\")\n",
    "        # Use model.generate() for text generation\n",
    "with torch.no_grad():\n",
    "    output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_new_tokens, # Set max total length\n",
    "    num_return_sequences=1,\n",
    "    compile_config=None,\n",
    "    do_sample=True, # Use sampling for more creative output\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id # Ensure generation stops correctly\n",
    "    )\n",
    "\n",
    "# 3. Decode and Print Output\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(\"-\" * 20)\n",
    "print(f\"Generated Text:\\n{generated_text}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc81ea-3565-418e-b35f-d1c275b0b300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
