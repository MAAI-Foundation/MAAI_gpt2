October 5, 2025
This repository contains models finetuned for Marathi. OUr goal is to publish open models that can be run on mobile and PC class devices and we'll be releaseing several in coming days. These models will be fully open sources so you'll have access to data and model.

There are three models we are publishing here today: gpt2_base, gpt2_medium, gpt2_large.
Neither of these models were trained for Marathi out of the box. When prompted in Marathi, they produced output in Hindi and English (That was a surprise to me as they were not trained in Hindi either - at least officially). 

The base model is more or less meaningless for any practical purposes as it has only 124 million parameters. In general, it didn't produce any meaningful output. The best accuracy for the training set topped at 83% but test and validation was limited to 62%. 
It is included here for the sake of completeness.

The medium model starts performing better. Our model does well in Marathi - scores high, and produces meaningful output. With more diverse dataset, should be usable on phone for certain applications. 

The large model and extra large models are best suited for practical applications. They generates text near flawlessly that can be used in many applications. This one has 774 million parameters, so one can fit this in ~1.6GB RAM, unquantized.

The need of the hour is good datasets for Marathi and that is what we are working on. We welcome participation from students, professionals and educational institutions to build better data sets so more models can be trained.

Feel free to reach out at anandjoshi1 at gmail dot com with your questions. 

Update Oct9, 2025
- Added inference scripts in each of the gpt directories. The script works well on Colab. The only issues I ran into were with Torch installation.
- Updated weights file.
- Updated base Keras file. Use this to learn.

Note: We would like to express gratitude to AMD for providing GPUs for this project. The PyTorch instance worked flawlessly. Thank you!
